---
layout: work
title: Research
slug: /research
items:
  - title: Fine$-$tuning LLM for Alzheimer’s Disease Diagnosis and Progression Prediction
   image:
      src: /assets/img/work/sand.png
      alt: sand
    description:  We investigated different serialized ways (e.g. Markdown, plain text, feature-wise, and visit-wise) for longitudinal tabular data from ADNI and HABS-HD as LLM inputs and finetuned Llama 3 and Llama 3.1 tailored to Alzheimer’s disease outcomes prediction. We are working on developing a statistical metric to construct an alpha-level confidence set to characterize the variable importance under the LLM context.

  - title: Mediation Analysis with Mendelian Randomization and Efficient Multiple GWAS Integration
    image:
      src: /assets/img/work/water.png
      alt: water
    description: We used structural equations to construct the relationship between mediator, exposure, and outcome effect based on the causal diagram. A three-step procedure was designed for conducting mediation analysis with integrated multiple GWAS using joint rerandomization and Rao-blackwellization to eliminate the **measurement error bias**, **the winner's curse**, **the loser's curse**, and **the imperfect IV selection issue**. See <a href="https://arxiv.org/abs/2312.10563"> preprint </a>, links to <a href="https://github.com/LQRrrrr/MAGIC"> code </a> and <a href="https://github.com/LQRrrrr/MR.Rerand"> package </a>.
  - title: On the Theoretical Investigation of Mediation Analysis with Mendelian Randomization and Summary Data
    image:
      src: /assets/img/work/sand.png
      alt: sand
    description: We provide rigorous statistical analysis of existing two popular frameworks for conducting mediation analysis with Mendelian Randomization. See <a href="https://drive.google.com/file/d/1kk7PRwMGYdazYJ7uE_MpJzFosRn3mWxi/view"> preprint </a>.
  
  - title: Benchmark of different QTL pipelines (including isoform-QTL, eQTL, and splicing-QTL)
    image:
      src: /assets/img/work/water.png
      alt: sand
    description: We compared the performance of RSEM, Kallisto, Cufflinks, Salmon + FastQTL, eQTL, and Leafcutter on the simulated dataset. We empirically demonstrated isoform-QTL pipelines outperform all others. Among all isoform-QTL pipelines, Cufflinks has the best performance in terms of power and false discovery rate. See  <a href="https://drive.google.com/file/d/1CQuQivzTD9LEZt5vPYFq9fZhVUDJVb_6/view?usp=sharing"> slides</a> (preparing Manuscript).
  
  - title: GMS training framework and WMMLP
    image:
      src: /assets/img/work/sand.png
      alt: sand
    description: We constructed the weighted multiplicative MLP (WMMLP) in PyTorch based on Taylor expansion of M estimators and used neural networks to solve M-estimation problem under the bootstrap and cross validation context. See  <a href="https://drive.google.com/file/d/1hN_bLWVfeioHlpYY2CtSDO2_Hv24f_6w/view?usp=sharing"> final summer research report</a>.  
---


If interested, please refer to <strong><a href="https://drive.google.com/file/d/1YCf2JEATVFkbi0evi93oLjf6YHmA1z-q/view?usp=sharing" target="_blank"> Qiuran's CV </a></strong> or contact me for more details.
<br />
<br />
